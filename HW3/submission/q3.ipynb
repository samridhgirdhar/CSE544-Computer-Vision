{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a284808",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0b41645",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, glob, zipfile, json, math\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# HuggingFace\n",
    "from transformers import (\n",
    "    BlipProcessor,\n",
    "    BlipForConditionalGeneration,\n",
    "    CLIPProcessor,\n",
    "    CLIPModel,\n",
    ")\n",
    "\n",
    "DEVICE = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "EXTRACT_DIR = \"samples\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1485230d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 images\n"
     ]
    }
   ],
   "source": [
    "Path(EXTRACT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "IMAGE_PATHS = sorted(glob.glob(os.path.join(EXTRACT_DIR, \"*\")))\n",
    "print(f\"Found {len(IMAGE_PATHS)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05185be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BLIP captioning model …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 12:41:37.637663: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745066497.650223 3442470 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745066497.653993 3442470 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1745066497.665718 3442470 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745066497.665732 3442470 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745066497.665733 3442470 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745066497.665734 3442470 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-19 12:41:37.669943: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading BLIP captioning model …\")\n",
    "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "blip_model     = (\n",
    "    BlipForConditionalGeneration\n",
    "    .from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "    .to(DEVICE)\n",
    "    .eval()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2253fa7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning: 100%|██████████| 10/10 [00:02<00:00,  3.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  [\n",
      "    \"samples/ILSVRC2012_test_00000003.jpg\",\n",
      "    \"a small dog walking on a green carpet\"\n",
      "  ],\n",
      "  [\n",
      "    \"samples/ILSVRC2012_test_00000004.jpg\",\n",
      "    \"a small dog running across a green field\"\n",
      "  ],\n",
      "  [\n",
      "    \"samples/ILSVRC2012_test_00000018.jpg\",\n",
      "    \"a family sitting in a pool with a towel\"\n",
      "  ],\n",
      "  [\n",
      "    \"samples/ILSVRC2012_test_00000019.jpg\",\n",
      "    \"a small bird sitting on a plant\"\n",
      "  ],\n",
      "  [\n",
      "    \"samples/ILSVRC2012_test_00000022.jpg\",\n",
      "    \"a small dog standing on a stone ledge\"\n",
      "  ],\n",
      "  [\n",
      "    \"samples/ILSVRC2012_test_00000023.jpg\",\n",
      "    \"a man riding a bike down a wet street\"\n",
      "  ],\n",
      "  [\n",
      "    \"samples/ILSVRC2012_test_00000025.jpg\",\n",
      "    \"a brown butterfly sitting on a green plant\"\n",
      "  ],\n",
      "  [\n",
      "    \"samples/ILSVRC2012_test_00000026.jpg\",\n",
      "    \"a man in a suit and tie sitting on a couch\"\n",
      "  ],\n",
      "  [\n",
      "    \"samples/ILSVRC2012_test_00000030.jpg\",\n",
      "    \"a duck drinking water from a pond\"\n",
      "  ],\n",
      "  [\n",
      "    \"samples/ILSVRC2012_test_00000034.jpg\",\n",
      "    \"a coffee machine with two cups on it\"\n",
      "  ]\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_caption(img: Image.Image) -> str:\n",
    "    inputs = blip_processor(img, return_tensors=\"pt\").to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        out = blip_model.generate(**inputs, max_length=30)\n",
    "    return blip_processor.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "captions = {}\n",
    "for img_path in tqdm(IMAGE_PATHS, desc=\"Captioning\"):\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    captions[img_path] = generate_caption(img)\n",
    "\n",
    "print(json.dumps(list(captions.items())[:11], indent=2))  # preview first 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98b81e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_model     = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(DEVICE).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1951cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_similarity(img: Image.Image, text: str) -> float:\n",
    "    inputs = clip_processor(text=[text], images=img, return_tensors=\"pt\", padding=True).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        outs = clip_model(**inputs)\n",
    "    img_emb  = outs.image_embeds / outs.image_embeds.norm(dim=-1, keepdim=True)\n",
    "    txt_emb  = outs.text_embeds  / outs.text_embeds.norm(dim=-1, keepdim=True)\n",
    "    return float((img_emb * txt_emb).sum())\n",
    "\n",
    "def clip_score(sim: float) -> float:\n",
    "    \"\"\"\n",
    "    CLIPScore / CLIPS = cosine similarity * 100\n",
    "    (Hessel et al. 2021). No length penalty for simplicity.\n",
    "    \"\"\"\n",
    "    return sim * 100.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73a5d227",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating with CLIP: 100%|██████████| 10/10 [00:00<00:00, 29.88it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption</th>\n",
       "      <th>clip_cosine</th>\n",
       "      <th>clip_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ILSVRC2012_test_00000003.jpg</td>\n",
       "      <td>a small dog walking on a green carpet</td>\n",
       "      <td>0.315696</td>\n",
       "      <td>31.569633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ILSVRC2012_test_00000004.jpg</td>\n",
       "      <td>a small dog running across a green field</td>\n",
       "      <td>0.327133</td>\n",
       "      <td>32.713330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ILSVRC2012_test_00000018.jpg</td>\n",
       "      <td>a family sitting in a pool with a towel</td>\n",
       "      <td>0.313364</td>\n",
       "      <td>31.336388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ILSVRC2012_test_00000019.jpg</td>\n",
       "      <td>a small bird sitting on a plant</td>\n",
       "      <td>0.289393</td>\n",
       "      <td>28.939295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ILSVRC2012_test_00000022.jpg</td>\n",
       "      <td>a small dog standing on a stone ledge</td>\n",
       "      <td>0.310364</td>\n",
       "      <td>31.036389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ILSVRC2012_test_00000023.jpg</td>\n",
       "      <td>a man riding a bike down a wet street</td>\n",
       "      <td>0.308366</td>\n",
       "      <td>30.836561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ILSVRC2012_test_00000025.jpg</td>\n",
       "      <td>a brown butterfly sitting on a green plant</td>\n",
       "      <td>0.289159</td>\n",
       "      <td>28.915885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ILSVRC2012_test_00000026.jpg</td>\n",
       "      <td>a man in a suit and tie sitting on a couch</td>\n",
       "      <td>0.288955</td>\n",
       "      <td>28.895539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ILSVRC2012_test_00000030.jpg</td>\n",
       "      <td>a duck drinking water from a pond</td>\n",
       "      <td>0.305266</td>\n",
       "      <td>30.526629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ILSVRC2012_test_00000034.jpg</td>\n",
       "      <td>a coffee machine with two cups on it</td>\n",
       "      <td>0.279633</td>\n",
       "      <td>27.963299</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          image                                     caption  \\\n",
       "0  ILSVRC2012_test_00000003.jpg       a small dog walking on a green carpet   \n",
       "1  ILSVRC2012_test_00000004.jpg    a small dog running across a green field   \n",
       "2  ILSVRC2012_test_00000018.jpg     a family sitting in a pool with a towel   \n",
       "3  ILSVRC2012_test_00000019.jpg             a small bird sitting on a plant   \n",
       "4  ILSVRC2012_test_00000022.jpg       a small dog standing on a stone ledge   \n",
       "5  ILSVRC2012_test_00000023.jpg       a man riding a bike down a wet street   \n",
       "6  ILSVRC2012_test_00000025.jpg  a brown butterfly sitting on a green plant   \n",
       "7  ILSVRC2012_test_00000026.jpg  a man in a suit and tie sitting on a couch   \n",
       "8  ILSVRC2012_test_00000030.jpg           a duck drinking water from a pond   \n",
       "9  ILSVRC2012_test_00000034.jpg        a coffee machine with two cups on it   \n",
       "\n",
       "   clip_cosine  clip_score  \n",
       "0     0.315696   31.569633  \n",
       "1     0.327133   32.713330  \n",
       "2     0.313364   31.336388  \n",
       "3     0.289393   28.939295  \n",
       "4     0.310364   31.036389  \n",
       "5     0.308366   30.836561  \n",
       "6     0.289159   28.915885  \n",
       "7     0.288955   28.895539  \n",
       "8     0.305266   30.526629  \n",
       "9     0.279633   27.963299  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = []\n",
    "for img_path, caption in tqdm(captions.items(), desc=\"Evaluating with CLIP\"):\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    cos   = clip_similarity(image, caption)\n",
    "    score = clip_score(cos)\n",
    "    rows.append(\n",
    "        dict(\n",
    "            image=os.path.basename(img_path),\n",
    "            caption=caption,\n",
    "            clip_cosine=cos,\n",
    "            clip_score=score,\n",
    "        )\n",
    "    )\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(\"blip_clip_results.csv\", index=False)\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6606fc",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "| Metric | What it measures | Good for | Caveats |\n",
    "|---|---|---|---|\n",
    "| **Cosine similarity (raw CLIP)** | Angular distance between CLIP image & text embeddings | Quick sanity‑check of semantic match; ranking captions for one image | Uncalibrated; values vary with model / layer; not directly comparable across setups |\n",
    "| **CLIPScore / CLIPS** | Cosine × 100 (sometimes length‑penalized) | Reporting caption quality with a single number; correlates well with human judgment | Still inherits CLIP bias; higher isn’t always better for specificity vs. generality |\n",
    "| **CIDEr** | n‑gram TF‑IDF similarity against multiple references | Traditional caption benchmarks (COCO, Flickr); rewards consensus wording | Needs ground‑truth reference captions—unavailable for web images or zero‑shot tasks |\n",
    "| **SPICE** | Scene‑graph overlap (objects, attributes, relations) | Evaluating semantic correctness beyond surface wording | Slower; depends on reliable scene‑graph parsing; again needs reference captions |\n",
    "| **BLEU / ROUGE / METEOR** | n‑gram overlap | Historical baselines; cheap to compute | Weak correlation with human judgment, especially for open‑vocabulary captions |\n",
    "| **Image–Text Retrieval Recall (R@k)** | Does the caption retrieve its own image among distractors? | Dataset‑level evaluation of alignment models | Requires a large gallery; only yields set‑level statistics, not per‑caption scores |\n",
    "| **Human evaluation** | Direct judgment of relevance, fluency, detail | Final QA, user‑facing applications | Expensive and slow; subjective variance |\n",
    "\n",
    "**When to use what**\n",
    "\n",
    "- **Exploratory or zero‑shot settings** (no reference captions): CLIP cosine / CLIPScore are handy—immediate, reference‑free, and correlate reasonably with human assessments.\n",
    "- **Model development on COCO‑style datasets**: pair CLIPScore with CIDEr or SPICE, so you capture both semantic alignment and lexical diversity.\n",
    "- **Application‑specific tuning** (e.g., product search captions): perform **retrieval recall**—does the caption uniquely find its image among similar items?\n",
    "- **Deployment‑critical outputs** (medical, legal): always add a round of **human evaluation**, even if automated scores look high.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sg_ip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
