{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402d7ecd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import os, sys, glob, argparse, shutil, cv2, numpy as np, torch\n",
    "from types import SimpleNamespace\n",
    "\n",
    "# ------- user editable paths --------------------------------------------------\n",
    "ROOT_IMG_DIR = \"datasets/my_two_image_folders\"   # <‑‑ EDIT\n",
    "OUTPUT_DIR   = \"outputs/matcher_vis\"             # <‑‑ EDIT\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# 1) minimal set of Matcher imports (assumes script lives in repo root)\n",
    "sys.path.append(\"./\")\n",
    "from matcher.Matcher import build_matcher_oss\n",
    "\n",
    "def overlay_mask(img, mask, alpha=0.6):\n",
    "    \"\"\"RGBA overlay for pretty visualisation (green mask).\"\"\"\n",
    "    overlay        = img.copy()\n",
    "    overlay[mask]  = (0,255,0)                   # green\n",
    "    return cv2.addWeighted(overlay, alpha, img, 1-alpha, 0)\n",
    "\n",
    "# 2) build a ready‑to‑use matcher instance (no training!)\n",
    "def get_matcher(device=\"cuda\"):\n",
    "    args = SimpleNamespace(\n",
    "        device            = torch.device(device if torch.cuda.is_available() else \"cpu\"),\n",
    "        # weight locations — keep default filenames\n",
    "        dinov2_weights    = \"models/dinov2_vitl14_pretrain.pth\",\n",
    "        sam_weights       = \"models/sam_vit_h_4b8939.pth\",\n",
    "        dinov2_size       = \"vit_large\",\n",
    "        sam_size          = \"vit_h\",\n",
    "        use_semantic_sam  = False,               # flip to True if you want part masks\n",
    "        # run‑time knobs (leave as defaults for simple images)\n",
    "        points_per_side   = 64,\n",
    "        pred_iou_thresh   = 0.88,\n",
    "        sel_stability_score_thresh = 0.90,\n",
    "        iou_filter        = 0.0,\n",
    "        box_nms_thresh    = 1.0,\n",
    "        output_layer      = 3,\n",
    "        use_dense_mask    = 0,\n",
    "        multimask_output  = 0,\n",
    "        num_centers       = 8,\n",
    "        use_box           = False,\n",
    "        use_points_or_centers = True,\n",
    "        sample_range      = (1,6),\n",
    "        max_sample_iterations = 64,\n",
    "        alpha=1.0, beta=0.0, exp=0.0,\n",
    "        emd_filter=0.0, purity_filter=0.0, coverage_filter=0.0,\n",
    "        use_score_filter=False, deep_score_filter=0.33,\n",
    "        deep_score_norm_filter=0.10, topk_scores_threshold=0.0,\n",
    "        num_merging_mask = 9,\n",
    "    )\n",
    "    return build_matcher_oss(args)\n",
    "\n",
    "matcher = get_matcher()\n",
    "\n",
    "# 3) walk through folders\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "for folder in sorted(os.listdir(ROOT_IMG_DIR)):\n",
    "    fpath = os.path.join(ROOT_IMG_DIR, folder)\n",
    "    if not os.path.isdir(fpath): continue\n",
    "\n",
    "    imgs = sorted(glob.glob(os.path.join(fpath, \"*.*g\")))   # jpg / png\n",
    "    assert len(imgs) == 2, f\"Folder {folder} must contain exactly two images\"\n",
    "    ref_img_p, tgt_img_p = imgs                              # (A,B)\n",
    "\n",
    "    for direction, (ref_p, tgt_p) in enumerate([(ref_img_p, tgt_img_p),\n",
    "                                                (tgt_img_p, ref_img_p)]):\n",
    "        ref = cv2.cvtColor(cv2.imread(ref_p), cv2.COLOR_BGR2RGB)\n",
    "        tgt = cv2.cvtColor(cv2.imread(tgt_p), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # ---- 1. build masks for the reference image automatically ------------\n",
    "        # SamAutomaticMaskGenerator (inside Matcher) returns K candidate masks.\n",
    "        # We use the **largest** area mask as reference; this works well for the\n",
    "        # “simple” images described in the assignment.\n",
    "        masks_ref = matcher.generator.generate(ref)\n",
    "        ref_mask  = max(masks_ref, key=lambda m: m[\"area\"])[\"segmentation\"]\n",
    "\n",
    "        # ---- 2. feed into Matcher -------------------------------------------\n",
    "        matcher.set_reference(torch.from_numpy(ref).permute(2,0,1).unsqueeze(0),\n",
    "                              torch.from_numpy(ref_mask[None, None, ...]))  # 1x1xH xW\n",
    "        matcher.set_target(torch.from_numpy(tgt).permute(2,0,1).unsqueeze(0))\n",
    "        with torch.no_grad():\n",
    "            pred_mask = matcher.predict().squeeze(0).bool().cpu().numpy()\n",
    "\n",
    "        matcher.clear()     # free CUDA memory for next pair\n",
    "\n",
    "        # ---- 3. save visualisations & raw mask ------------------------------\n",
    "        out_sub   = os.path.join(OUTPUT_DIR, folder)\n",
    "        os.makedirs(out_sub, exist_ok=True)\n",
    "\n",
    "        # 3.a pretty overlay\n",
    "        vis = overlay_mask(cv2.cvtColor(tgt, cv2.COLOR_RGB2BGR), pred_mask)\n",
    "        cv2.imwrite(os.path.join(out_sub,\n",
    "                    f\"seg_{direction+1}_{os.path.basename(tgt_p)}\"), vis)\n",
    "\n",
    "        # 3.b binary mask for quantitative use\n",
    "        np.save(os.path.join(out_sub,\n",
    "                 f\"mask_{direction+1}_{os.path.splitext(os.path.basename(tgt_p))[0]}.npy\"),\n",
    "                pred_mask.astype(np.uint8))\n",
    "\n",
    "        print(f\"[✓] {folder}: {os.path.basename(ref_p)} → {os.path.basename(tgt_p)}\")\n",
    "\n",
    "print(f\"\\nAll done!  Results live under:  {OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46bcf5d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# #!/usr/bin/env python3\n",
    "# \"\"\"\n",
    "# one_shot_box_segment.py\n",
    "\n",
    "# For each two‑image folder under ROOT:\n",
    "#   - A→B: use A’s largest SAM mask to prompt a box on B → save seg_1_B, mask_1_B\n",
    "#   - B→A: same in reverse         → save seg_2_A, mask_2_A\n",
    "# \"\"\"\n",
    "\n",
    "# import os\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# from pathlib import Path\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
    "\n",
    "# # ── USER CONFIG ───────────────────────────────────\n",
    "# ROOT     = Path(\"Images\")               # parent folder of subfolders\n",
    "# OUTDIR   = Path(\"outputs/sam_box\")      # where to save results\n",
    "# SAM_W    = \"models/sam_vit_h_4b8939.pth\"       # SAM weights\n",
    "# IMG_SIDE = 518                          # SAM’s fixed input size\n",
    "# # ──────────────────────────────────────────────────\n",
    "\n",
    "# device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# # 1) load SAM\n",
    "# sam = sam_model_registry[\"vit_h\"](checkpoint=SAM_W).to(device)\n",
    "# sam.eval()\n",
    "\n",
    "# # Automatic mask generator (for reference)\n",
    "# auto_gen = SamAutomaticMaskGenerator(sam,\n",
    "#     points_per_side=64,\n",
    "#     pred_iou_thresh=0.88,\n",
    "#     stability_score_thresh=0.95\n",
    "# )\n",
    "\n",
    "# # Predictor (for box prompt on target)\n",
    "# predictor = SamPredictor(sam)\n",
    "\n",
    "# def get_largest_box(rgb_uint8: np.ndarray):\n",
    "#     \"\"\"Return bounding box [x0,y0,x1,y1] of SAM’s largest mask.\"\"\"\n",
    "#     masks = auto_gen.generate(rgb_uint8)\n",
    "#     seg  = max(masks, key=lambda m: m[\"area\"])[\"segmentation\"]  # H×W bool\n",
    "#     ys, xs = np.where(seg)\n",
    "#     return [int(xs.min()), int(ys.min()), int(xs.max()), int(ys.max())]\n",
    "\n",
    "# def segment_with_box(rgb_uint8: np.ndarray, box: list):\n",
    "#     \"\"\"\n",
    "#     Prompt SAM with box→ returns a boolean mask H×W.\n",
    "#     rgb_uint8 must be original image resized to IMG_SIDE.\n",
    "#     \"\"\"\n",
    "#     predictor.set_image(rgb_uint8)  \n",
    "#     masks, _, _ = predictor.predict(\n",
    "#         box=np.array(box)[None, :],       # 1×4\n",
    "#         multimask_output=False\n",
    "#     )\n",
    "#     return masks[0]  # H×W bool\n",
    "\n",
    "# def overlay(bgr, mask, alpha=0.6):\n",
    "#     o = bgr.copy()\n",
    "#     o[mask] = (0,255,0)\n",
    "#     return cv2.addWeighted(o, alpha, bgr, 1-alpha, 0)\n",
    "\n",
    "# # make output dirs\n",
    "# OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "# mask_gen = SamAutomaticMaskGenerator(\n",
    "#     sam,\n",
    "#     points_per_side=16,        # speed vs quality trade‑off\n",
    "#     pred_iou_thresh=0.88,\n",
    "#     stability_score_thresh=0.95\n",
    "# )\n",
    "\n",
    "\n",
    "# for sub in tqdm(sorted(p for p in ROOT.iterdir() if p.is_dir())):\n",
    "#     imgs = sorted(sub.glob(\"*.*g\"))\n",
    "#     assert len(imgs) == 2, f\"{sub} must have exactly two images\"\n",
    "#     A, B = imgs\n",
    "#     dest = OUTDIR / sub.name\n",
    "#     dest.mkdir(exist_ok=True)\n",
    "\n",
    "#     for idx, (ref_p, tgt_p) in enumerate([(A,B),(B,A)], start=1):\n",
    "#         # load & resize (uint8)\n",
    "#         ref0 = cv2.cvtColor(cv2.imread(str(ref_p)), cv2.COLOR_BGR2RGB)\n",
    "#         tgt0 = cv2.cvtColor(cv2.imread(str(tgt_p)), cv2.COLOR_BGR2RGB)\n",
    "#         ref  = cv2.resize(ref0, (IMG_SIDE, IMG_SIDE), interpolation=cv2.INTER_AREA)\n",
    "#         tgt  = cv2.resize(tgt0, (IMG_SIDE, IMG_SIDE), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "#         # 1) box from ref’s largest mask\n",
    "#         box = get_largest_box(ref)\n",
    "\n",
    "#         # 2) segment on target with that box\n",
    "#         mask = segment_with_box(tgt, box)\n",
    "\n",
    "#         # 3) save overlay + raw mask\n",
    "#         vis = overlay(\n",
    "#             cv2.cvtColor(tgt, cv2.COLOR_RGB2BGR),\n",
    "#             mask\n",
    "#         )\n",
    "#         seg_name  = dest / f\"seg_{idx}_{tgt_p.name}\"\n",
    "#         mask_name = dest / f\"mask_{idx}_{tgt_p.stem}.npy\"\n",
    "#         cv2.imwrite(str(seg_name), vis)\n",
    "#         np.save(str(mask_name), mask.astype(np.uint8))\n",
    "\n",
    "#     print(f\"[✓] {sub.name}\")\n",
    "\n",
    "# print(f\"\\nDone! Results in: {OUTDIR}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
