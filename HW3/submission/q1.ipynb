{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da0920f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Extract results\n",
    "clip_scores = [5.95, 78.37, 0.14, 0.01, 1.21, 0.02, 0.02, 0.00, 0.63, 13.62]\n",
    "clips_scores = [76.11, 16.04, 0.01, 0.00, 1.57, 0.00, 0.00, 0.03, 0.77, 5.46]\n",
    "labels = [f\"Caption {i+1}\" for i in range(10)]\n",
    "\n",
    "# Create plot\n",
    "x = np.arange(len(labels))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "rects1 = ax.bar(x - width/2, clip_scores, width, label='CLIP')\n",
    "rects2 = ax.bar(x + width/2, clips_scores, width, label='CLIPS')\n",
    "\n",
    "# Add labels and legend\n",
    "ax.set_xlabel('Captions')\n",
    "ax.set_ylabel('Similarity Score (%)')\n",
    "ax.set_title('Comparison of CLIP vs CLIPS Similarity Scores')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b5c8e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2559d8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: The install method you used for conda--probably either `pip install conda`\n",
      "or `easy_install conda`--is not compatible with using conda as an application.\n",
      "If your intention is to install conda as a standalone application, currently\n",
      "supported install methods include the Anaconda installer and the miniconda\n",
      "installer.  You can download the miniconda installer from\n",
      "https://conda.io/miniconda.html.\n",
      "\n",
      "Collecting ftfy\n",
      "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: regex in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (4.67.1)\n",
      "Requirement already satisfied: wcwidth in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from ftfy) (0.2.13)\n",
      "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
      "Installing collected packages: ftfy\n",
      "Successfully installed ftfy-6.3.1\n",
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-z0h9jzf9\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-z0h9jzf9\n",
      "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: ftfy in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from clip==1.0) (6.3.1)\n",
      "Requirement already satisfied: packaging in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from clip==1.0) (24.2)\n",
      "Requirement already satisfied: regex in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from clip==1.0) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from clip==1.0) (4.67.1)\n",
      "Requirement already satisfied: torch in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from clip==1.0) (2.6.0)\n",
      "Requirement already satisfied: torchvision in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from clip==1.0) (0.21.0)\n",
      "Requirement already satisfied: wcwidth in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from ftfy->clip==1.0) (0.2.13)\n",
      "Requirement already satisfied: filelock in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from torch->clip==1.0) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from torch->clip==1.0) (4.13.0)\n",
      "Requirement already satisfied: networkx in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from torch->clip==1.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from torch->clip==1.0) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from torch->clip==1.0) (2024.12.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from torch->clip==1.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from torch->clip==1.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from torch->clip==1.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from torch->clip==1.0) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from torch->clip==1.0) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from torch->clip==1.0) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from torch->clip==1.0) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from torch->clip==1.0) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from torch->clip==1.0) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from torch->clip==1.0) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from torch->clip==1.0) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from torch->clip==1.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from torch->clip==1.0) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from torch->clip==1.0) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from torch->clip==1.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\n",
      "Requirement already satisfied: numpy in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from torchvision->clip==1.0) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from torchvision->clip==1.0) (11.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from jinja2->torch->clip==1.0) (3.0.2)\n",
      "Building wheels for collected packages: clip\n",
      "  Building wheel for clip (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369514 sha256=3e4805154f61a7051a81179e6d0303150c85618a93c485092b829a3ef7c14271\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-vid0yqou/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n",
      "Successfully built clip\n",
      "Installing collected packages: clip\n",
      "Successfully installed clip-1.0\n"
     ]
    }
   ],
   "source": [
    "!conda install --yes -c pytorch pytorch=1.7.1 torchvision cudatoolkit=11.0\n",
    "!pip install ftfy regex tqdm\n",
    "!pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931d8301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CLIP Similarity Scores:\n",
      "A man holding a large dog: 5.95%\n",
      "A gray Great Dane being held by its owner: 78.37%\n",
      "A person with a small puppy: 0.14%\n",
      "A dog sitting on a couch: 0.01%\n",
      "A man in formal attire with a pet: 1.21%\n",
      "A woman holding a cat: 0.02%\n",
      "A gray horse in a stable: 0.02%\n",
      "A person standing next to a bookshelf: 0.00%\n",
      "A large dog with its owner in a living room: 0.63%\n",
      "A man in a white shirt holding a gray dog: 13.62%\n"
     ]
    }
   ],
   "source": [
    "# Install required dependencies\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "image = Image.open(\"sample_image.jpg\")\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "\n",
    "image_input = preprocess(image).unsqueeze(0).to(device)\n",
    "\n",
    "\n",
    "captions = [\n",
    "    \"A man holding a large dog\",\n",
    "    \"A gray Great Dane being held by its owner\",\n",
    "    \"A person with a small puppy\",\n",
    "    \"A dog sitting on a couch\",\n",
    "    \"A man in formal attire with a pet\",\n",
    "    \"A woman holding a cat\",\n",
    "    \"A gray horse in a stable\",\n",
    "    \"A person standing next to a bookshelf\",\n",
    "    \"A large dog with its owner in a living room\",\n",
    "    \"A man in a white shirt holding a gray dog\"\n",
    "]\n",
    "\n",
    "\n",
    "text_inputs = torch.cat([clip.tokenize(c) for c in captions]).to(device)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image_input)\n",
    "    text_features = model.encode_text(text_inputs)\n",
    "    \n",
    "\n",
    "    image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "\n",
    "    similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "    \n",
    "    \n",
    "    print(\"\\nCLIP Similarity Scores:\")\n",
    "    for i, caption in enumerate(captions):\n",
    "        print(f\"{caption}: {similarity[0][i].item():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b41f57b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch>=1.9.0 in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (2.6.0)\n",
      "Requirement already satisfied: torchvision in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (0.21.0)\n",
      "Requirement already satisfied: regex in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (2024.11.6)\n",
      "Requirement already satisfied: ftfy in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (6.3.1)\n",
      "Requirement already satisfied: tqdm in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (4.67.1)\n",
      "Requirement already satisfied: huggingface_hub in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (0.30.2)\n",
      "Requirement already satisfied: safetensors in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (0.5.3)\n",
      "Requirement already satisfied: timm in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (1.0.15)\n",
      "Requirement already satisfied: transformers in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from -r requirements.txt (line 9)) (4.52.0.dev0)\n",
      "Requirement already satisfied: filelock in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from torch>=1.9.0->-r requirements.txt (line 1)) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from torch>=1.9.0->-r requirements.txt (line 1)) (4.13.0)\n",
      "Requirement already satisfied: networkx in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from torch>=1.9.0->-r requirements.txt (line 1)) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from torch>=1.9.0->-r requirements.txt (line 1)) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from torch>=1.9.0->-r requirements.txt (line 1)) (2024.12.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from torch>=1.9.0->-r requirements.txt (line 1)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from torch>=1.9.0->-r requirements.txt (line 1)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from torch>=1.9.0->-r requirements.txt (line 1)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from torch>=1.9.0->-r requirements.txt (line 1)) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from torch>=1.9.0->-r requirements.txt (line 1)) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from torch>=1.9.0->-r requirements.txt (line 1)) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from torch>=1.9.0->-r requirements.txt (line 1)) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from torch>=1.9.0->-r requirements.txt (line 1)) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from torch>=1.9.0->-r requirements.txt (line 1)) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from torch>=1.9.0->-r requirements.txt (line 1)) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from torch>=1.9.0->-r requirements.txt (line 1)) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from torch>=1.9.0->-r requirements.txt (line 1)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from torch>=1.9.0->-r requirements.txt (line 1)) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from torch>=1.9.0->-r requirements.txt (line 1)) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from torch>=1.9.0->-r requirements.txt (line 1)) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.9.0->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: numpy in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from torchvision->-r requirements.txt (line 2)) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from torchvision->-r requirements.txt (line 2)) (11.1.0)\n",
      "Requirement already satisfied: wcwidth in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from ftfy->-r requirements.txt (line 4)) (0.2.13)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from huggingface_hub->-r requirements.txt (line 6)) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from huggingface_hub->-r requirements.txt (line 6)) (6.0.2)\n",
      "Requirement already satisfied: requests in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from huggingface_hub->-r requirements.txt (line 6)) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 9)) (0.21.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from jinja2->torch>=1.9.0->-r requirements.txt (line 1)) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from requests->huggingface_hub->-r requirements.txt (line 6)) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from requests->huggingface_hub->-r requirements.txt (line 6)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from requests->huggingface_hub->-r requirements.txt (line 6)) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from requests->huggingface_hub->-r requirements.txt (line 6)) (2025.1.31)\n",
      "Collecting open_clip_torch\n",
      "  Downloading open_clip_torch-2.32.0-py3-none-any.whl.metadata (31 kB)\n",
      "Requirement already satisfied: torch>=1.9.0 in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from open_clip_torch) (2.6.0)\n",
      "Requirement already satisfied: torchvision in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from open_clip_torch) (0.21.0)\n",
      "Requirement already satisfied: regex in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from open_clip_torch) (2024.11.6)\n",
      "Requirement already satisfied: ftfy in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from open_clip_torch) (6.3.1)\n",
      "Requirement already satisfied: tqdm in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from open_clip_torch) (4.67.1)\n",
      "Requirement already satisfied: huggingface-hub in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from open_clip_torch) (0.30.2)\n",
      "Requirement already satisfied: safetensors in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from open_clip_torch) (0.5.3)\n",
      "Requirement already satisfied: timm in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from open_clip_torch) (1.0.15)\n",
      "Requirement already satisfied: filelock in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (4.13.0)\n",
      "Requirement already satisfied: networkx in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (2024.12.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.9.0->open_clip_torch) (1.3.0)\n",
      "Requirement already satisfied: wcwidth in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from ftfy->open_clip_torch) (0.2.13)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from huggingface-hub->open_clip_torch) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from huggingface-hub->open_clip_torch) (6.0.2)\n",
      "Requirement already satisfied: requests in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from huggingface-hub->open_clip_torch) (2.32.3)\n",
      "Requirement already satisfied: numpy in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from torchvision->open_clip_torch) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from torchvision->open_clip_torch) (11.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from jinja2->torch>=1.9.0->open_clip_torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from requests->huggingface-hub->open_clip_torch) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from requests->huggingface-hub->open_clip_torch) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from requests->huggingface-hub->open_clip_torch) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/iiitd/miniconda3/envs/sg_ip/lib/python3.10/site-packages (from requests->huggingface-hub->open_clip_torch) (2025.1.31)\n",
      "Downloading open_clip_torch-2.32.0-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: open_clip_torch\n",
      "Successfully installed open_clip_torch-2.32.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -r requirements.txt\n",
    "!pip install open_clip_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5f5f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3398974/2893933456.py:30: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), torch.cuda.amp.autocast() if torch.cuda.is_available() else torch.no_grad():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CLIPS Similarity Scores:\n",
      "A man holding a large dog: 76.11%\n",
      "A gray Great Dane being held by its owner: 16.04%\n",
      "A person with a small puppy: 0.01%\n",
      "A dog sitting on a couch: 0.00%\n",
      "A man in formal attire with a pet: 1.57%\n",
      "A woman holding a cat: 0.00%\n",
      "A gray horse in a stable: 0.00%\n",
      "A person standing next to a bookshelf: 0.03%\n",
      "A large dog with its owner in a living room: 0.77%\n",
      "A man in a white shirt holding a gray dog: 5.46%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from open_clip import create_model_from_pretrained, get_tokenizer\n",
    "\n",
    "\n",
    "model, preprocess = create_model_from_pretrained('hf-hub:UCSC-VLAA/ViT-L-14-CLIPS-Recap-DataComp-1B')\n",
    "tokenizer = get_tokenizer('hf-hub:UCSC-VLAA/ViT-L-14-CLIPS-Recap-DataComp-1B')\n",
    "\n",
    "\n",
    "image = Image.open(\"sample_image.jpg\")\n",
    "image_input = preprocess(image).unsqueeze(0)\n",
    "\n",
    "\n",
    "captions = [\n",
    "    \"A man holding a large dog\",\n",
    "    \"A gray Great Dane being held by its owner\",\n",
    "    \"A person with a small puppy\",\n",
    "    \"A dog sitting on a couch\",\n",
    "    \"A man in formal attire with a pet\",\n",
    "    \"A woman holding a cat\",\n",
    "    \"A gray horse in a stable\",\n",
    "    \"A person standing next to a bookshelf\",\n",
    "    \"A large dog with its owner in a living room\",\n",
    "    \"A man in a white shirt holding a gray dog\"\n",
    "]\n",
    "\n",
    "\n",
    "with torch.no_grad(), torch.cuda.amp.autocast() if torch.cuda.is_available() else torch.no_grad():\n",
    "    \n",
    "    text_tokens = tokenizer(captions, context_length=model.context_length)\n",
    "    \n",
    "    \n",
    "    image_features = model.encode_image(image_input)\n",
    "    text_features = model.encode_text(text_tokens)\n",
    "    \n",
    "    \n",
    "    image_features = F.normalize(image_features, dim=-1)\n",
    "    text_features = F.normalize(text_features, dim=-1)\n",
    "    \n",
    "    \n",
    "    similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "    \n",
    "    \n",
    "    print(\"\\nCLIPS Similarity Scores:\")\n",
    "    for i, caption in enumerate(captions):\n",
    "        print(f\"{caption}: {similarity[0][i].item():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b507dec8",
   "metadata": {},
   "source": [
    "# Analysis of CLIP vs. CLIPS Results\n",
    "\n",
    "Looking at the similarity scores from both models, there are several interesting patterns and differences worth noting:\n",
    "\n",
    "## Key Observations\n",
    "\n",
    "1. **Different Top Predictions:**\n",
    "   - CLIP strongly favors \"A gray Great Dane being held by its owner\" (78.37%)\n",
    "   - CLIPS strongly favors \"A man holding a large dog\" (76.11%)\n",
    "\n",
    "2. **Specificity vs. Generality:**\n",
    "   - CLIP gives higher scores to more specific descriptions (breed identification)\n",
    "   - CLIPS gives higher scores to more general but accurate descriptions\n",
    "\n",
    "3. **Secondary Preference:**\n",
    "   - CLIP's second choice is \"A man in a white shirt holding a gray dog\" (13.62%)\n",
    "   - CLIPS's second choice is \"A gray Great Dane being held by its owner\" (16.04%)\n",
    "\n",
    "4. **Similar Rejections:**\n",
    "   - Both models correctly assign very low probabilities to obviously incorrect descriptions like \"A woman holding a cat\" and \"A gray horse in a stable\"\n",
    "\n",
    "5. **Background Element Recognition:**\n",
    "   - Both models assign low probability to \"A person standing next to a bookshelf\" despite the bookshelf being visible, suggesting they prioritize the main subjects\n",
    "\n",
    "## Model Behavior Analysis\n",
    "\n",
    "**CLIP** appears to be more confident in specific visual details like the breed identification (Great Dane) and color attributes. This suggests CLIP may be better at fine-grained visual categorization when trained on web-crawled pairs that often contain specific nomenclature.\n",
    "\n",
    "**CLIPS** seems to prioritize the overall scene description and relationship between subjects (\"man holding large dog\") over specific breed identification. This could reflect its training on synthetic captions that might focus more on relationships and actions rather than specific taxonomic labels.\n",
    "\n",
    "## Practical Implications\n",
    "\n",
    "These differences highlight how model training approaches affect what visual-textual relationships are emphasized:\n",
    "\n",
    "1. **Use Case Considerations:**\n",
    "   - CLIP might be preferable for applications requiring fine-grained categorization or specific attribute recognition\n",
    "   - CLIPS might be better for applications focused on understanding relationships and actions between subjects\n",
    "\n",
    "2. **Caption Style Preferences:**\n",
    "   - CLIP appears to favor descriptive, taxonomically precise captions\n",
    "   - CLIPS appears to favor action-oriented, relationship-focused captions\n",
    "\n",
    "3. **Error Patterns:**\n",
    "   - Both models effectively reject completely incorrect descriptions\n",
    "   - Both models assign low probabilities to background elements that are present but not the main focus\n",
    "\n",
    "This comparison demonstrates that while both models perform the same fundamental task (visual-language alignment), their different architectures and training approaches result in notably different prioritization of visual information. CLIP's emphasis on specific visual features versus CLIPS's emphasis on relationships and actions highlights the importance of model selection based on specific application requirements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166f0eb5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sg_ip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
